#!/usr/bin/env python3
"""
Weekly Analysis and Export for EmoJournal Bot
Generates insights and CSV exports based on user data
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import csv
import io
from collections import Counter

from .i18n import Texts, format_emotion_list, get_time_period_text, generate_insight

logger = logging.getLogger(__name__)

class WeeklyAnalyzer:
    """Analyzes user emotion data and generates insights"""
    
    def __init__(self, db):
        self.db = db
        self.texts = Texts()
    
    async def generate_summary(self, user_id: int, days: int = 7) -> str:
        """Generate summary for user - works with any number of entries"""
        try:
            entries = self.db.get_user_entries(user_id, days)
            
            # –£–±–∏—Ä–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ - —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–ø–∏—Å–µ–π
            if len(entries) == 0:
                return self.texts.NO_DATA_MESSAGE
            
            if len(entries) == 1:
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏
                entry = entries[0]
                emotions = self._parse_emotions(entry.emotions) if entry.emotions else ['–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ']
                return f"""üìä <b>–¢–≤–æ—è –ø–µ—Ä–≤–∞—è –∑–∞–ø–∏—Å—å!</b>

<b>üé≠ –≠–º–æ—Ü–∏—è:</b> {', '.join(emotions)}

<b>üîç –ü—Ä–∏—á–∏–Ω–∞:</b> {entry.cause if entry.cause else '–Ω–µ —É–∫–∞–∑–∞–Ω–∞'}

<b>‚è∞ –í—Ä–µ–º—è:</b> {entry.ts_local.strftime('%H:%M')} ({get_time_period_text(entry.ts_local.hour)})

<b>üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:</b> 1

üí° <b>–û—Ç–ª–∏—á–Ω–æ!</b> –¢—ã —Å–¥–µ–ª–∞–ª(–∞) –ø–µ—Ä–≤—ã–π —à–∞–≥ –∫ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç–∏. –ü—Ä–æ–¥–æ–ª–∂–∞–π –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å —ç–º–æ—Ü–∏–∏, –∏ –≤—Å–∫–æ—Ä–µ —è —Å–º–æ–≥—É –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏!

<i>–ò—Å–ø–æ–ª—å–∑—É–π /note –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π.</i>"""
            
            # –ê–Ω–∞–ª–∏–∑ –¥–ª—è 2+ –∑–∞–ø–∏—Å–µ–π
            emotion_freq = self._analyze_emotions(entries)
            top_emotions = self._get_top_items(emotion_freq, limit=5)
            
            trigger_freq = self._analyze_triggers(entries)
            top_triggers = self._get_top_items(trigger_freq, limit=5)
            
            time_dist = self._analyze_time_distribution(entries)
            peak_hour = max(time_dist.items(), key=lambda x: x[1])[0] if time_dist else 12
            peak_period = get_time_period_text(peak_hour)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã (–¥–ª—è 2+ –∑–∞–ø–∏—Å–µ–π –∏–Ω—Å–∞–π—Ç—ã –º–µ–Ω–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ)
            if len(entries) >= 5:
                insights = generate_insight(top_emotions, top_triggers, peak_hour)
            else:
                insights = self._generate_simple_insights(entries, top_emotions)
            
            # –§–æ—Ä–º–∞—Ç —Å–≤–æ–¥–∫–∏
            summary = self.texts.WEEKLY_SUMMARY_TEMPLATE.format(
                top_emotions=format_emotion_list(top_emotions),
                top_triggers=format_emotion_list(top_triggers) if top_triggers else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
                peak_hours=f"{peak_hour:02d}:00 ({peak_period})",
                total_entries=len(entries),
                insights=insights
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"Failed to generate summary for user {user_id}: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–¥–∫—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def _generate_simple_insights(self, entries, top_emotions) -> str:
        """Generate simple insights for small number of entries"""
        if len(entries) < 2:
            return ""
        
        insights = []
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
        if len(entries) == 2:
            insights.append("üí° <b>–ù–∞—á–∞–ª–æ –ø—É—Ç–∏:</b> –£ —Ç–µ–±—è —É–∂–µ 2 –∑–∞–ø–∏—Å–∏! –ü—Ä–æ–¥–æ–ª–∂–∞–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —ç–º–æ—Ü–∏–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.")
        elif len(entries) == 3:
            insights.append("üí° <b>–•–æ—Ä–æ—à–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å:</b> 3 –∑–∞–ø–∏—Å–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–≤–∏–¥–µ—Ç—å –ø–µ—Ä–≤—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤ —Ç–≤–æ—ë–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏.")
        elif len(entries) == 4:
            insights.append("üí° <b>–§–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∫–∞—Ä—Ç–∏–Ω–∞:</b> 4 –∑–∞–ø–∏—Å–∏ –¥–∞—é—Ç –±–æ–ª–µ–µ —è—Å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–≤–æ–∏—Ö —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö.")
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–∏—Ö —ç–º–æ—Ü–∏–π
        if top_emotions:
            top_emotion = top_emotions[0][0] if isinstance(top_emotions[0], tuple) else top_emotions[0]
            
            positive_emotions = {'—Ä–∞–¥–æ—Å—Ç—å', '—Å—á–∞—Å—Ç—å–µ', '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ', '—Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ', '—ç–Ω–µ—Ä–≥–∏—è'}
            negative_emotions = {'—Ç—Ä–µ–≤–æ–≥–∞', '–≥—Ä—É—Å—Ç—å', '–∑–ª–æ—Å—Ç—å', '—É—Å—Ç–∞–ª–æ—Å—Ç—å', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ'}
            
            if top_emotion in positive_emotions:
                insights.append("‚ú® –ó–¥–æ—Ä–æ–≤–æ, —á—Ç–æ –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏!")
            elif top_emotion in negative_emotions:
                insights.append("ü§ó –ó–∞–º–µ—á–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —ç–º–æ—Ü–∏–∏ ‚Äî –≤–∞–∂–Ω—ã–π —à–∞–≥ –∫ –∏—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—é.")
        
        return "\n\n".join(insights)
    
    def _analyze_emotions(self, entries) -> Dict[str, int]:
        """Extract and count emotion frequencies with normalization"""
        emotion_counts = Counter()
        
        for entry in entries:
            if entry.emotions:
                emotions = self._parse_emotions(entry.emotions)
                for emotion in emotions:
                    normalized = self._normalize_emotion(emotion)
                    if normalized:
                        emotion_counts[normalized] += 1
        
        return dict(emotion_counts)
    
    def _parse_emotions(self, emotions_str: str) -> List[str]:
        """Parse emotions from JSON or plain text"""
        if not emotions_str:
            return []
        
        try:
            # Try parsing as JSON array
            emotions = json.loads(emotions_str)
            if isinstance(emotions, list):
                return [str(e).strip().lower() for e in emotions if e]
        except (json.JSONDecodeError, TypeError):
            pass
        
        # Fall back to plain text parsing
        return [emotions_str.strip().lower()] if emotions_str.strip() else []
    
    def _normalize_emotion(self, emotion: str) -> Optional[str]:
        """Normalize emotion to base form (simple stemming for Russian)"""
        emotion = emotion.strip().lower()
        
        if len(emotion) < 2:
            return None
        
        # Simple Russian emotion normalization rules
        emotion_mapping = {
            # –†–∞–¥–æ—Å—Ç—å family
            '—Ä–∞–¥–æ—Å—Ç–Ω—ã–π': '—Ä–∞–¥–æ—Å—Ç—å',
            '—Ä–∞–¥–æ—Å—Ç–Ω–∞—è': '—Ä–∞–¥–æ—Å—Ç—å', 
            '—Ä–∞–¥–æ—Å—Ç–Ω–æ–µ': '—Ä–∞–¥–æ—Å—Ç—å',
            '—Å—á–∞—Å—Ç–ª–∏–≤—ã–π': '—Å—á–∞—Å—Ç—å–µ',
            '—Å—á–∞—Å—Ç–ª–∏–≤–∞—è': '—Å—á–∞—Å—Ç—å–µ',
            '–¥–æ–≤–æ–ª—å–Ω—ã–π': '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ',
            '–¥–æ–≤–æ–ª—å–Ω–∞—è': '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ',
            
            # –¢—Ä–µ–≤–æ–≥–∞ family  
            '—Ç—Ä–µ–≤–æ–∂–Ω—ã–π': '—Ç—Ä–µ–≤–æ–≥–∞',
            '—Ç—Ä–µ–≤–æ–∂–Ω–∞—è': '—Ç—Ä–µ–≤–æ–≥–∞',
            '–±–µ—Å–ø–æ–∫–æ–π–Ω—ã–π': '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', 
            '–±–µ—Å–ø–æ–∫–æ–π–Ω–∞—è': '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ',
            '–Ω–µ—Ä–≤–Ω—ã–π': '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å',
            '–Ω–µ—Ä–≤–Ω–∞—è': '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å',
            
            # –ì—Ä—É—Å—Ç—å family
            '–≥—Ä—É—Å—Ç–Ω—ã–π': '–≥—Ä—É—Å—Ç—å',
            '–≥—Ä—É—Å—Ç–Ω–∞—è': '–≥—Ä—É—Å—Ç—å',
            '–ø–µ—á–∞–ª—å–Ω—ã–π': '–ø–µ—á–∞–ª—å',
            '–ø–µ—á–∞–ª—å–Ω–∞—è': '–ø–µ—á–∞–ª—å',
            
            # –ó–ª–æ—Å—Ç—å family
            '–∑–ª–æ–π': '–∑–ª–æ—Å—Ç—å',
            '–∑–ª–∞—è': '–∑–ª–æ—Å—Ç—å', 
            '—Ä–∞–∑–¥—Ä–∞–∂—ë–Ω–Ω—ã–π': '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ',
            '—Ä–∞–∑–¥—Ä–∞–∂—ë–Ω–Ω–∞—è': '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ',
            
            # –£—Å—Ç–∞–ª–æ—Å—Ç—å family
            '—É—Å—Ç–∞–ª—ã–π': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
            '—É—Å—Ç–∞–ª–∞—è': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
            '—É—Å—Ç–∞–≤—à–∏–π': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
            '—É—Å—Ç–∞–≤—à–∞—è': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
        }
        
        # Direct mapping
        if emotion in emotion_mapping:
            return emotion_mapping[emotion]
        
        # Remove common Russian adjective endings
        for ending in ['—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–æ–π', '–µ–π', '—ë–Ω', '–Ω–∞', '–Ω–æ', '–Ω—ã']:
            if emotion.endswith(ending) and len(emotion) > len(ending) + 2:
                base = emotion[:-len(ending)]
                # Check if base form exists in our emotion categories
                for category in self.texts.EMOTION_CATEGORIES.values():
                    if base in category['emotions']:
                        return base
        
        return emotion
    
    def _analyze_triggers(self, entries) -> Dict[str, int]:
        """Extract and count trigger/cause frequencies"""
        trigger_counts = Counter()
        
        # Russian stop words to filter out
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∏–∑', '–∫', '–æ—Ç', '—É', '–æ', 
            '–∑–∞', '–ø—Ä–∏', '–¥–æ', '–ø–æ—Å–ª–µ', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '–Ω–∞–¥', '–ø–æ–¥',
            '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è',
            '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Å–µ–π—á–∞—Å', '–ø–æ—Ç–æ–º',
            '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–±—É–¥–µ—Ç', '—Å—Ç–∞–ª', '—Å—Ç–∞–ª–∞'
        }
        
        for entry in entries:
            if entry.cause:
                cause_text = entry.cause.lower().strip()
                
                # Extract meaningful words (simple keyword extraction)
                words = cause_text.replace(',', ' ').replace('.', ' ').split()
                
                for word in words:
                    word = word.strip('.,!?;:()[]{}"\'-')
                    
                    # Filter short words and stop words
                    if len(word) >= 3 and word not in stop_words:
                        trigger_counts[word] += 1
        
        return dict(trigger_counts)
    
    def _analyze_time_distribution(self, entries) -> Dict[int, int]:
        """Analyze distribution by hour of day"""
        hour_counts = Counter()
        
        for entry in entries:
            hour = entry.ts_local.hour
            hour_counts[hour] += 1
        
        return dict(hour_counts)
    
    def _get_top_items(self, frequency_dict: Dict[str, int], limit: int = 5) -> List[Tuple[str, int]]:
        """Get top N items by frequency"""
        if not frequency_dict:
            return []
        
        return sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True)[:limit]
    
    async def export_csv(self, user_id: int) -> Optional[str]:
        """Export user data as CSV string"""
        try:
            entries = self.db.get_user_entries(user_id, days=365)  # Export all data
            
            if not entries:
                return None
            
            output = io.StringIO()
            writer = csv.writer(output)
            
            # Write headers
            writer.writerow(self.texts.CSV_HEADERS)
            
            # Write data rows
            for entry in entries:
                # Parse emotions
                emotions = self._parse_emotions(entry.emotions) if entry.emotions else []
                emotions_str = ', '.join(emotions)
                
                # Parse tags
                tags = []
                if entry.tags:
                    try:
                        tags = json.loads(entry.tags)
                    except json.JSONDecodeError:
                        tags = [entry.tags]
                tags_str = ', '.join(tags) if tags else ''
                
                # Format row
                row = [
                    entry.ts_local.strftime('%Y-%m-%d'),  # –î–∞—Ç–∞
                    entry.ts_local.strftime('%H:%M'),     # –í—Ä–µ–º—è
                    entry.valence or '',                   # –í–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å
                    entry.arousal or '',                   # –ê–∫—Ç–∏–≤–∞—Ü–∏—è  
                    emotions_str,                          # –≠–º–æ—Ü–∏–∏
                    entry.cause or '',                     # –ü—Ä–∏—á–∏–Ω–∞
                    entry.body or '',                      # –¢–µ–ª–µ—Å–Ω—ã–µ –æ—â—É—â–µ–Ω–∏—è
                    entry.note or '',                      # –ó–∞–º–µ—Ç–∫–∞
                    tags_str                               # –¢–µ–≥–∏
                ]
                
                writer.writerow(row)
            
            return output.getvalue()
            
        except Exception as e:
            logger.error(f"Failed to export CSV for user {user_id}: {e}")
            return None
    
    def get_emotion_timeline(self, user_id: int, days: int = 30) -> List[Dict]:
        """Get emotion timeline for visualization"""
        entries = self.db.get_user_entries(user_id, days)
        
        timeline = []
        for entry in entries:
            emotions = self._parse_emotions(entry.emotions) if entry.emotions else []
            
            timeline.append({
                'timestamp': entry.ts_local.isoformat(),
                'date': entry.ts_local.strftime('%Y-%m-%d'),
                'time': entry.ts_local.strftime('%H:%M'),
                'emotions': emotions,
                'valence': entry.valence,
                'arousal': entry.arousal,
                'cause': entry.cause,
                'note': entry.note
            })
        
        return timeline


def test_analyzer():
    """Test the analyzer with sample data"""
    from .db import Database, Entry
    import tempfile
    import os
    
    # Create test database
    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:
        test_db_path = f.name
    
    try:
        db = Database(f'sqlite:///{test_db_path}')
        analyzer = WeeklyAnalyzer(db)
        
        # Create test user
        user = db.create_user(12345, 67890)
        
        # Test with 1 entry
        db.create_entry(12345, emotions='["—Ä–∞–¥–æ—Å—Ç—å"]', cause='—Ö–æ—Ä–æ—à–∏–π –¥–µ–Ω—å')
        
        import asyncio
        summary = asyncio.run(analyzer.generate_summary(12345))
        print("Summary with 1 entry:")
        print(summary)
        print("\n" + "="*50 + "\n")
        
        # Add more entries
        test_entries = [
            {'emotions': '["—Ç—Ä–µ–≤–æ–≥–∞"]', 'cause': '–º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã'},  
            {'emotions': '["—Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ"]', 'cause': '–≤–µ—á–µ—Ä –¥–æ–º–∞'},
            {'emotions': '["—É—Å—Ç–∞–ª–æ—Å—Ç—å"]', 'cause': '–¥–æ–ª–≥–∏–π –¥–µ–Ω—å'},
        ]
        
        for entry_data in test_entries:
            db.create_entry(12345, **entry_data)
        
        # Test with multiple entries
        summary = asyncio.run(analyzer.generate_summary(12345))
        print("Summary with 4 entries:")
        print(summary)
        
        print("\nAnalyzer tests passed!")
        
    finally:
        os.unlink(test_db_path)


if __name__ == "__main__":
    test_analyzer()
