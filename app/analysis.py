#!/usr/bin/env python3
"""
Weekly Analysis and Export for EmoJournal Bot
Generates insights and CSV exports based on user data
Enhanced with emotion categorization
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import csv
import io
from collections import Counter, defaultdict

from .i18n import Texts, format_emotion_list, get_time_period_text, generate_insight

logger = logging.getLogger(__name__)

class WeeklyAnalyzer:
    """Analyzes user emotion data and generates insights with categorization"""
    
    # –ù–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç–º–æ—Ü–∏–π
    EMOTION_GROUPS = {
        'growth': {
            'name': 'üå± –≠–º–æ—Ü–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ —Ä–æ—Å—Ç–∞',
            'categories': ['joy', 'interest', 'calm']
        },
        'tension': {
            'name': 'üå™ –≠–º–æ—Ü–∏–∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –∏ —Å–∏–≥–Ω–∞–ª–∞', 
            'categories': ['anxiety', 'sadness', 'anger', 'shame', 'fatigue']
        },
        'neutral': {
            'name': '‚öñ –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ / –ø—Ä–æ—á–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è',
            'categories': ['excitement']  # –∏ –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ
        }
    }
    
    def __init__(self, db):
        self.db = db
        self.texts = Texts()
    
    async def generate_summary(self, user_id: int, days: int = 7) -> str:
        """Generate enhanced summary with emotion grouping"""
        try:
            entries = self.db.get_user_entries(user_id, days)
            
            if len(entries) == 0:
                return self.texts.NO_DATA_MESSAGE
            
            if len(entries) == 1:
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏
                entry = entries[0]
                emotions = self._parse_emotions(entry.emotions) if entry.emotions else ['–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ']
                return f"""üìä <b>–¢–≤–æ—è –ø–µ—Ä–≤–∞—è –∑–∞–ø–∏—Å—å!</b>

<b>üé≠ –≠–º–æ—Ü–∏—è:</b> {', '.join(emotions)}

<b>üîç –ü—Ä–∏—á–∏–Ω–∞:</b> {entry.cause if entry.cause else '–Ω–µ —É–∫–∞–∑–∞–Ω–∞'}

<b>‚è∞ –í—Ä–µ–º—è:</b> {entry.ts_local.strftime('%H:%M')} ({get_time_period_text(entry.ts_local.hour)})

<b>üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:</b> 1

üí° <b>–û—Ç–ª–∏—á–Ω–æ!</b> –¢—ã —Å–¥–µ–ª–∞–ª(–∞) –ø–µ—Ä–≤—ã–π —à–∞–≥ –∫ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç–∏. –ü—Ä–æ–¥–æ–ª–∂–∞–π –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å —ç–º–æ—Ü–∏–∏, –∏ –≤—Å–∫–æ—Ä–µ —è —Å–º–æ–≥—É –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏!

<i>–ò—Å–ø–æ–ª—å–∑—É–π /note –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π.</i>"""
            
            # –ê–Ω–∞–ª–∏–∑ –¥–ª—è 2+ –∑–∞–ø–∏—Å–µ–π
            emotion_analysis = self._analyze_emotions_by_groups(entries)
            trigger_analysis = self._analyze_triggers_by_groups(entries)
            
            time_dist = self._analyze_time_distribution(entries)
            peak_hour = max(time_dist.items(), key=lambda x: x[1])[0] if time_dist else 12
            peak_period = get_time_period_text(peak_hour)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
            insights = self._generate_enhanced_insights(entries, emotion_analysis, trigger_analysis)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—É—é —Å–≤–æ–¥–∫—É
            summary = self._format_enhanced_summary(
                emotion_analysis, 
                trigger_analysis, 
                peak_hour, 
                peak_period, 
                len(entries), 
                insights
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"Failed to generate summary for user {user_id}: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–¥–∫—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def _analyze_emotions_by_groups(self, entries) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –ø–æ –Ω–æ–≤—ã–º –≥—Ä—É–ø–ø–∞–º"""
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ —ç–º–æ—Ü–∏–∏ –∫–∞–∫ —Ä–∞–Ω—å—à–µ
        emotion_freq = self._analyze_emotions(entries)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        grouped_emotions = {
            'growth': defaultdict(int),
            'tension': defaultdict(int), 
            'neutral': defaultdict(int)
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —ç–º–æ—Ü–∏–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º
        for emotion, count in emotion_freq.items():
            group = self._get_emotion_group(emotion)
            grouped_emotions[group][emotion] += count
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {}
        for group_key, group_info in self.EMOTION_GROUPS.items():
            emotions_in_group = dict(grouped_emotions[group_key])
            total_count = sum(emotions_in_group.values())
            top_emotions = sorted(emotions_in_group.items(), key=lambda x: x[1], reverse=True)[:3]
            
            result[group_key] = {
                'name': group_info['name'],
                'total_count': total_count,
                'emotions': emotions_in_group,
                'top_emotions': top_emotions
            }
        
        return result
    
    def _analyze_triggers_by_groups(self, entries) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –ø–æ –≥—Ä—É–ø–ø–∞–º —ç–º–æ—Ü–∏–π"""
        # –°–æ–±–∏—Ä–∞–µ–º —Ç—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —ç–º–æ—Ü–∏–π
        grouped_triggers = {
            'growth': [],
            'tension': [],
            'neutral': []
        }
        
        for entry in entries:
            if entry.cause and entry.emotions:
                emotions = self._parse_emotions(entry.emotions)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É —ç–º–æ—Ü–∏–π –¥–ª—è —ç—Ç–æ–π –∑–∞–ø–∏—Å–∏
                emotion_groups = [self._get_emotion_group(emotion) for emotion in emotions]
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å —ç–º–æ—Ü–∏–∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è, —Ç—Ä–∏–≥–≥–µ—Ä –æ—Ç–Ω–æ—Å–∏–º –∫ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—é
                if 'tension' in emotion_groups:
                    grouped_triggers['tension'].append(entry.cause)
                # –ï—Å–ª–∏ –µ—Å—Ç—å —ç–º–æ—Ü–∏–∏ —Ä–æ—Å—Ç–∞, —Ç—Ä–∏–≥–≥–µ—Ä –æ—Ç–Ω–æ—Å–∏–º –∫ —Ä–æ—Å—Ç—É
                elif 'growth' in emotion_groups:
                    grouped_triggers['growth'].append(entry.cause)
                # –ò–Ω–∞—á–µ –∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º
                else:
                    grouped_triggers['neutral'].append(entry.cause)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {}
        for group_key, group_info in self.EMOTION_GROUPS.items():
            triggers = grouped_triggers[group_key]
            result[group_key] = {
                'name': group_info['name'].replace('–≠–º–æ—Ü–∏–∏', '–¢—Ä–∏–≥–≥–µ—Ä—ã —ç–º–æ—Ü–∏–π'),
                'triggers': triggers,
                'count': len(triggers)
            }
        
        return result
    
    def _get_emotion_group(self, emotion: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥—Ä—É–ø–ø—É —ç–º–æ—Ü–∏–∏"""
        emotion = emotion.lower().strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        for group_key, group_info in self.EMOTION_GROUPS.items():
            for category in group_info['categories']:
                category_emotions = self.texts.EMOTION_CATEGORIES.get(category, {}).get('emotions', [])
                if emotion in category_emotions:
                    return group_key
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è
        return 'neutral'
    
    def _format_enhanced_summary(self, emotion_analysis, trigger_analysis, peak_hour, peak_period, total_entries, insights):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–π —Å–≤–æ–¥–∫–∏"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –±–ª–æ–∫–∏ —ç–º–æ—Ü–∏–π
        emotion_blocks = []
        for group_key in ['growth', 'tension', 'neutral']:
            group_data = emotion_analysis[group_key]
            if group_data['total_count'] > 0:
                top_emotions_str = ', '.join([f"{emotion} ({count})" for emotion, count in group_data['top_emotions']])
                emotion_blocks.append(f"<b>{group_data['name']}:</b> {group_data['total_count']} —Ä–∞–∑\n{top_emotions_str}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –±–ª–æ–∫–∏ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
        trigger_blocks = []
        for group_key in ['growth', 'tension', 'neutral']:
            group_data = trigger_analysis[group_key]
            if group_data['count'] > 0:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ç—Ä–∏–≥–≥–µ—Ä–∞ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä—ã
                sample_triggers = group_data['triggers'][:3]
                triggers_str = '; '.join(sample_triggers)
                if len(group_data['triggers']) > 3:
                    triggers_str += f" (–∏ –µ—â—ë {len(group_data['triggers']) - 3})"
                trigger_blocks.append(f"<b>{group_data['name']}:</b>\n{triggers_str}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É
        summary_parts = [
            "üìä <b>–¢–≤–æ—è –Ω–µ–¥–µ–ª—è –≤ —ç–º–æ—Ü–∏—è—Ö</b>\n"
        ]
        
        if emotion_blocks:
            summary_parts.append("<b>üé≠ –≠–º–æ—Ü–∏–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º:</b>")
            summary_parts.extend(emotion_blocks)
            summary_parts.append("")
        
        if trigger_blocks:
            summary_parts.append("<b>üîç –ß—Ç–æ –≤–ª–∏—è–ª–æ –Ω–∞ —ç–º–æ—Ü–∏–∏:</b>")
            summary_parts.extend(trigger_blocks)
            summary_parts.append("")
        
        summary_parts.extend([
            f"<b>‚è∞ –ü–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:</b> {peak_hour:02d}:00 ({peak_period})",
            f"<b>üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:</b> {total_entries}",
            ""
        ])
        
        if insights:
            summary_parts.append(insights)
            summary_parts.append("")
        
        summary_parts.append("<i>–•–æ—á–µ—à—å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏? –ò—Å–ø–æ–ª—å–∑—É–π /export –¥–ª—è CSV-—Ñ–∞–π–ª–∞.</i>")
        
        return "\n".join(summary_parts)
    
    def _generate_enhanced_insights(self, entries, emotion_analysis, trigger_analysis):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
        insights = []
        
        # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ —ç–º–æ—Ü–∏–π
        growth_count = emotion_analysis['growth']['total_count']
        tension_count = emotion_analysis['tension']['total_count']
        total_emotional = growth_count + tension_count
        
        if total_emotional > 0:
            growth_ratio = growth_count / total_emotional
            
            if growth_ratio >= 0.7:
                insights.append("‚ú® <b>–û—Ç–ª–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å!</b> –ü—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç —ç–º–æ—Ü–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ —Ä–æ—Å—Ç–∞.")
            elif growth_ratio >= 0.4:
                insights.append("‚öñÔ∏è <b>–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–µ–¥–µ–ª—è:</b> —ç–º–æ—Ü–∏–∏ —Ä–æ—Å—Ç–∞ –∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –≤ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–∏.")
            else:
                insights.append("ü§ó <b>–ù–µ–ø—Ä–æ—Å—Ç–∞—è –Ω–µ–¥–µ–ª—è:</b> –º–Ω–æ–≥–æ —ç–º–æ—Ü–∏–π –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - –æ–Ω–∏ —Ç–æ–∂–µ –≤–∞–∂–Ω—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–µ–±—è.")
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if len(entries) >= 5:
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ —Ä–æ—Å—Ç–∞
            growth_triggers = trigger_analysis['growth']['triggers']
            if len(growth_triggers) >= 2:
                insights.append(f"üí° <b>–ß—Ç–æ —Ç–µ–±—è –≤–¥–æ—Ö–Ω–æ–≤–ª—è–µ—Ç:</b> –æ–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–∏—Ç—É–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–æ—Å—è—Ç —ç–º–æ—Ü–∏–∏ —Ä–æ—Å—Ç–∞.")
            
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è
            tension_triggers = trigger_analysis['tension']['triggers']
            if len(tension_triggers) >= 2:
                insights.append(f"üõ°Ô∏è <b>–ó–æ–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è:</b> —Å—Ç–æ–∏—Ç –ø–æ–¥—É–º–∞—Ç—å –æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö —Ä–∞–±–æ—Ç—ã —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è —Å—Ç—Ä–µ—Å—Å–æ—Ä–∞–º–∏.")
        
        return "\n\n".join(insights) if insights else ""
    
    def _generate_simple_insights(self, entries, emotion_analysis) -> str:
        """Generate simple insights for small number of entries"""
        if len(entries) < 2:
            return ""
        
        insights = []
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
        if len(entries) == 2:
            insights.append("üí° <b>–ù–∞—á–∞–ª–æ –ø—É—Ç–∏:</b> –£ —Ç–µ–±—è —É–∂–µ 2 –∑–∞–ø–∏—Å–∏! –ü—Ä–æ–¥–æ–ª–∂–∞–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —ç–º–æ—Ü–∏–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.")
        elif len(entries) == 3:
            insights.append("üí° <b>–•–æ—Ä–æ—à–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å:</b> 3 –∑–∞–ø–∏—Å–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–≤–∏–¥–µ—Ç—å –ø–µ—Ä–≤—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤ —Ç–≤–æ—ë–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏.")
        elif len(entries) == 4:
            insights.append("üí° <b>–§–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∫–∞—Ä—Ç–∏–Ω–∞:</b> 4 –∑–∞–ø–∏—Å–∏ –¥–∞—é—Ç –±–æ–ª–µ–µ —è—Å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–≤–æ–∏—Ö —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö.")
        
        # –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø —ç–º–æ—Ü–∏–π
        growth_count = emotion_analysis['growth']['total_count']
        tension_count = emotion_analysis['tension']['total_count']
        
        if growth_count > tension_count:
            insights.append("‚ú® –ó–¥–æ—Ä–æ–≤–æ, —á—Ç–æ –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏!")
        elif tension_count > growth_count:
            insights.append("ü§ó –ó–∞–º–µ—á–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —ç–º–æ—Ü–∏–∏ ‚Äî –≤–∞–∂–Ω—ã–π —à–∞–≥ –∫ –∏—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—é.")
        
        return "\n\n".join(insights)
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –ª–æ–≥–∏–∫—É
    def _analyze_emotions(self, entries) -> Dict[str, int]:
        """Extract and count emotion frequencies with normalization"""
        emotion_counts = Counter()
        
        for entry in entries:
            if entry.emotions:
                emotions = self._parse_emotions(entry.emotions)
                for emotion in emotions:
                    normalized = self._normalize_emotion(emotion)
                    if normalized:
                        emotion_counts[normalized] += 1
        
        return dict(emotion_counts)
    
    def _parse_emotions(self, emotions_str: str) -> List[str]:
        """Parse emotions from JSON or plain text"""
        if not emotions_str:
            return []
        
        try:
            # Try parsing as JSON array
            emotions = json.loads(emotions_str)
            if isinstance(emotions, list):
                return [str(e).strip().lower() for e in emotions if e]
        except (json.JSONDecodeError, TypeError):
            pass
        
        # Fall back to plain text parsing
        return [emotions_str.strip().lower()] if emotions_str.strip() else []
    
    def _normalize_emotion(self, emotion: str) -> Optional[str]:
        """Normalize emotion to base form (simple stemming for Russian)"""
        emotion = emotion.strip().lower()
        
        if len(emotion) < 2:
            return None
        
        # Simple Russian emotion normalization rules
        emotion_mapping = {
            # –†–∞–¥–æ—Å—Ç—å family
            '—Ä–∞–¥–æ—Å—Ç–Ω—ã–π': '—Ä–∞–¥–æ—Å—Ç—å',
            '—Ä–∞–¥–æ—Å—Ç–Ω–∞—è': '—Ä–∞–¥–æ—Å—Ç—å', 
            '—Ä–∞–¥–æ—Å—Ç–Ω–æ–µ': '—Ä–∞–¥–æ—Å—Ç—å',
            '—Å—á–∞—Å—Ç–ª–∏–≤—ã–π': '—Å—á–∞—Å—Ç—å–µ',
            '—Å—á–∞—Å—Ç–ª–∏–≤–∞—è': '—Å—á–∞—Å—Ç—å–µ',
            '–¥–æ–≤–æ–ª—å–Ω—ã–π': '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ',
            '–¥–æ–≤–æ–ª—å–Ω–∞—è': '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ',
            
            # –¢—Ä–µ–≤–æ–≥–∞ family  
            '—Ç—Ä–µ–≤–æ–∂–Ω—ã–π': '—Ç—Ä–µ–≤–æ–≥–∞',
            '—Ç—Ä–µ–≤–æ–∂–Ω–∞—è': '—Ç—Ä–µ–≤–æ–≥–∞',
            '–±–µ—Å–ø–æ–∫–æ–π–Ω—ã–π': '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', 
            '–±–µ—Å–ø–æ–∫–æ–π–Ω–∞—è': '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ',
            '–Ω–µ—Ä–≤–Ω—ã–π': '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å',
            '–Ω–µ—Ä–≤–Ω–∞—è': '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å',
            
            # –ì—Ä—É—Å—Ç—å family
            '–≥—Ä—É—Å—Ç–Ω—ã–π': '–≥—Ä—É—Å—Ç—å',
            '–≥—Ä—É—Å—Ç–Ω–∞—è': '–≥—Ä—É—Å—Ç—å',
            '–ø–µ—á–∞–ª—å–Ω—ã–π': '–ø–µ—á–∞–ª—å',
            '–ø–µ—á–∞–ª—å–Ω–∞—è': '–ø–µ—á–∞–ª—å',
            
            # –ó–ª–æ—Å—Ç—å family
            '–∑–ª–æ–π': '–∑–ª–æ—Å—Ç—å',
            '–∑–ª–∞—è': '–∑–ª–æ—Å—Ç—å', 
            '—Ä–∞–∑–¥—Ä–∞–∂—ë–Ω–Ω—ã–π': '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ',
            '—Ä–∞–∑–¥—Ä–∞–∂—ë–Ω–Ω–∞—è': '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ',
            
            # –£—Å—Ç–∞–ª–æ—Å—Ç—å family
            '—É—Å—Ç–∞–ª—ã–π': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
            '—É—Å—Ç–∞–ª–∞—è': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
            '—É—Å—Ç–∞–≤—à–∏–π': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
            '—É—Å—Ç–∞–≤—à–∞—è': '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
        }
        
        # Direct mapping
        if emotion in emotion_mapping:
            return emotion_mapping[emotion]
        
        # Remove common Russian adjective endings
        for ending in ['—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–æ–π', '–µ–π', '—ë–Ω', '–Ω–∞', '–Ω–æ', '–Ω—ã']:
            if emotion.endswith(ending) and len(emotion) > len(ending) + 2:
                base = emotion[:-len(ending)]
                # Check if base form exists in our emotion categories
                for category in self.texts.EMOTION_CATEGORIES.values():
                    if base in category['emotions']:
                        return base
        
        return emotion
    
    def _analyze_triggers(self, entries) -> Dict[str, int]:
        """Extract and count trigger/cause frequencies"""
        trigger_counts = Counter()
        
        # Russian stop words to filter out
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∏–∑', '–∫', '–æ—Ç', '—É', '–æ', 
            '–∑–∞', '–ø—Ä–∏', '–¥–æ', '–ø–æ—Å–ª–µ', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '–Ω–∞–¥', '–ø–æ–¥',
            '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è',
            '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Å–µ–π—á–∞—Å', '–ø–æ—Ç–æ–º',
            '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–±—É–¥–µ—Ç', '—Å—Ç–∞–ª', '—Å—Ç–∞–ª–∞'
        }
        
        for entry in entries:
            if entry.cause:
                cause_text = entry.cause.lower().strip()
                
                # Extract meaningful words (simple keyword extraction)
                words = cause_text.replace(',', ' ').replace('.', ' ').split()
                
                for word in words:
                    word = word.strip('.,!?;:()[]{}"\'-')
                    
                    # Filter short words and stop words
                    if len(word) >= 3 and word not in stop_words:
                        trigger_counts[word] += 1
        
        return dict(trigger_counts)
    
    def _analyze_time_distribution(self, entries) -> Dict[int, int]:
        """Analyze distribution by hour of day"""
        hour_counts = Counter()
        
        for entry in entries:
            hour = entry.ts_local.hour
            hour_counts[hour] += 1
        
        return dict(hour_counts)
    
    def _get_top_items(self, frequency_dict: Dict[str, int], limit: int = 5) -> List[Tuple[str, int]]:
        """Get top N items by frequency"""
        if not frequency_dict:
            return []
        
        return sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True)[:limit]
    
    async def export_csv(self, user_id: int) -> Optional[str]:
        """Export user data as CSV string"""
        try:
            entries = self.db.get_user_entries(user_id, days=365)  # Export all data
            
            if not entries:
                return None
            
            output = io.StringIO()
            writer = csv.writer(output)
            
            # Write headers
            writer.writerow(self.texts.CSV_HEADERS)
            
            # Write data rows
            for entry in entries:
                # Parse emotions
                emotions = self._parse_emotions(entry.emotions) if entry.emotions else []
                emotions_str = ', '.join(emotions)
                
                # Parse tags
                tags = []
                if entry.tags:
                    try:
                        tags = json.loads(entry.tags)
                    except json.JSONDecodeError:
                        tags = [entry.tags]
                tags_str = ', '.join(tags) if tags else ''
                
                # Format row
                row = [
                    entry.ts_local.strftime('%Y-%m-%d'),  # –î–∞—Ç–∞
                    entry.ts_local.strftime('%H:%M'),     # –í—Ä–µ–º—è
                    entry.valence or '',                   # –í–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å
                    entry.arousal or '',                   # –ê–∫—Ç–∏–≤–∞—Ü–∏—è  
                    emotions_str,                          # –≠–º–æ—Ü–∏–∏
                    entry.cause or '',                     # –ü—Ä–∏—á–∏–Ω–∞
                    entry.body or '',                      # –¢–µ–ª–µ—Å–Ω—ã–µ –æ—â—É—â–µ–Ω–∏—è
                    entry.note or '',                      # –ó–∞–º–µ—Ç–∫–∞
                    tags_str                               # –¢–µ–≥–∏
                ]
                
                writer.writerow(row)
            
            return output.getvalue()
            
        except Exception as e:
            logger.error(f"Failed to export CSV for user {user_id}: {e}")
            return None
    
    def get_emotion_timeline(self, user_id: int, days: int = 30) -> List[Dict]:
        """Get emotion timeline for visualization"""
        entries = self.db.get_user_entries(user_id, days)
        
        timeline = []
        for entry in entries:
            emotions = self._parse_emotions(entry.emotions) if entry.emotions else []
            
            timeline.append({
                'timestamp': entry.ts_local.isoformat(),
                'date': entry.ts_local.strftime('%Y-%m-%d'),
                'time': entry.ts_local.strftime('%H:%M'),
                'emotions': emotions,
                'valence': entry.valence,
                'arousal': entry.arousal,
                'cause': entry.cause,
                'note': entry.note
            })
        
        return timeline


def test_analyzer():
    """Test the analyzer with sample data"""
    from .db import Database, Entry
    import tempfile
    import os
    
    # Create test database
    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:
        test_db_path = f.name
    
    try:
        db = Database(f'sqlite:///{test_db_path}')
        analyzer = WeeklyAnalyzer(db)
        
        # Create test user
        user = db.create_user(12345, 67890)
        
        # Add test entries with different emotion groups
        test_entries = [
            {'emotions': '["—Ä–∞–¥–æ—Å—Ç—å"]', 'cause': '–∑–∞–∫–æ–Ω—á–∏–ª –ø—Ä–æ–µ–∫—Ç'},
            {'emotions': '["—Ç—Ä–µ–≤–æ–≥–∞"]', 'cause': '–º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã'},  
            {'emotions': '["—Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ"]', 'cause': '–≤–µ—á–µ—Ä –¥–æ–º–∞'},
            {'emotions': '["—É—Å—Ç–∞–ª–æ—Å—Ç—å"]', 'cause': '–¥–æ–ª–≥–∏–π –¥–µ–Ω—å'},
            {'emotions': '["–∏–Ω—Ç–µ—Ä–µ—Å"]', 'cause': '–Ω–æ–≤–∞—è –∫–Ω–∏–≥–∞'},
        ]
        
        for entry_data in test_entries:
            db.create_entry(12345, **entry_data)
        
        import asyncio
        summary = asyncio.run(analyzer.generate_summary(12345))
        print("Enhanced summary:")
        print(summary)
        
        print("\nEnhanced analyzer tests passed!")
        
    finally:
        os.unlink(test_db_path)


if __name__ == "__main__":
    test_analyzer()
